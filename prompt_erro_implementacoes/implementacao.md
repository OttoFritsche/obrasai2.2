# Implementa√ß√£o da Vectoriza√ß√£o da Documenta√ß√£o - ObrasAI

## Status Atual

A infraestrutura para vectoriza√ß√£o da documenta√ß√£o est√° planejada e parcialmente implementada, mas a vectoriza√ß√£o dos documentos de treinamento ainda n√£o foi executada. O sistema de IA contextual existe, mas n√£o tem os dados de treinamento processados para funcionar adequadamente.

## Pr√≥ximos Passos Necess√°rios

### 1. Criar a tabela embeddings_conhecimento no banco de dados

**Objetivo:** Criar a estrutura de dados para armazenar os embeddings dos documentos de treinamento.

**Implementa√ß√£o:**
```sql
-- Criar tabela para armazenar embeddings da documenta√ß√£o
CREATE TABLE embeddings_conhecimento (
  id UUID DEFAULT gen_random_uuid() PRIMARY KEY,
  documento VARCHAR(255) NOT NULL, -- Nome do documento (ex: 'documentacao_despesas')
  secao VARCHAR(255), -- Se√ß√£o espec√≠fica do documento
  conteudo TEXT NOT NULL, -- Texto original do chunk
  embedding VECTOR(1536), -- Embedding OpenAI (1536 dimens√µes)
  metadata JSONB, -- Metadados adicionais (tags, categoria, etc.)
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Criar √≠ndice para busca por similaridade
CREATE INDEX ON embeddings_conhecimento USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);

-- Criar √≠ndice para busca por documento
CREATE INDEX idx_embeddings_conhecimento_documento ON embeddings_conhecimento(documento);

-- Habilitar RLS
ALTER TABLE embeddings_conhecimento ENABLE ROW LEVEL SECURITY;

-- Pol√≠tica RLS para permitir leitura para usu√°rios autenticados
CREATE POLICY "Permitir leitura de embeddings para usu√°rios autenticados" 
ON embeddings_conhecimento FOR SELECT 
TO authenticated 
USING (true);
```

**Localiza√ß√£o:** Executar no Supabase SQL Editor ou via migration.

### 2. Implementar a Edge Function gerar-embeddings-documentacao

**Objetivo:** Criar fun√ß√£o serverless para processar documentos e gerar embeddings.

**Implementa√ß√£o:**

**Arquivo:** `supabase/functions/gerar-embeddings-documentacao/index.ts`

```typescript
import { serve } from "https://deno.land/std@0.168.0/http/server.ts"
import { createClient } from 'https://esm.sh/@supabase/supabase-js@2'

const corsHeaders = {
  'Access-Control-Allow-Origin': '*',
  'Access-Control-Allow-Headers': 'authorization, x-client-info, apikey, content-type',
}

interface DocumentChunk {
  documento: string;
  secao: string;
  conteudo: string;
  metadata?: any;
}

serve(async (req) => {
  if (req.method === 'OPTIONS') {
    return new Response('ok', { headers: corsHeaders })
  }

  try {
    const { documento, chunks } = await req.json() as {
      documento: string;
      chunks: DocumentChunk[];
    }

    if (!documento || !chunks || !Array.isArray(chunks)) {
      return new Response(
        JSON.stringify({ error: 'Documento e chunks s√£o obrigat√≥rios' }),
        { status: 400, headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
      )
    }

    const supabase = createClient(
      Deno.env.get('SUPABASE_URL') ?? '',
      Deno.env.get('SUPABASE_SERVICE_ROLE_KEY') ?? ''
    )

    const openaiApiKey = Deno.env.get('OPENAI_API_KEY')
    if (!openaiApiKey) {
      throw new Error('OPENAI_API_KEY n√£o configurada')
    }

    const embeddings = []

    // Processar cada chunk
    for (const chunk of chunks) {
      try {
        // Gerar embedding usando OpenAI
        const embeddingResponse = await fetch('https://api.openai.com/v1/embeddings', {
          method: 'POST',
          headers: {
            'Authorization': `Bearer ${openaiApiKey}`,
            'Content-Type': 'application/json',
          },
          body: JSON.stringify({
            model: 'text-embedding-ada-002',
            input: chunk.conteudo,
          }),
        })

        if (!embeddingResponse.ok) {
          console.error(`Erro ao gerar embedding para chunk: ${chunk.secao}`, await embeddingResponse.text())
          continue
        }

        const embeddingData = await embeddingResponse.json()
        const embedding = embeddingData.data[0].embedding

        embeddings.push({
          documento: chunk.documento,
          secao: chunk.secao,
          conteudo: chunk.conteudo,
          embedding,
          metadata: chunk.metadata || {}
        })

      } catch (error) {
        console.error(`Erro ao processar chunk ${chunk.secao}:`, error)
        continue
      }
    }

    // Salvar embeddings no banco
    if (embeddings.length > 0) {
      const { error: insertError } = await supabase
        .from('embeddings_conhecimento')
        .insert(embeddings)

      if (insertError) {
        console.error('Erro ao inserir embeddings:', insertError)
        throw insertError
      }
    }

    return new Response(
      JSON.stringify({ 
        success: true, 
        processados: embeddings.length,
        total_chunks: chunks.length
      }),
      { headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
    )

  } catch (error) {
    console.error('Erro na fun√ß√£o gerar-embeddings-documentacao:', error)
    return new Response(
      JSON.stringify({ error: error.message }),
      { status: 500, headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
    )
  }
})
```

**Configura√ß√£o necess√°ria:**
- Adicionar `OPENAI_API_KEY` nas vari√°veis de ambiente do Supabase
- Deploy da fun√ß√£o: `supabase functions deploy gerar-embeddings-documentacao`

### 3. Executar a vectoriza√ß√£o inicial dos documentos

**Objetivo:** Processar todos os documentos de treinamento e gerar embeddings.

**Implementa√ß√£o:**

**Arquivo:** `scripts/vectorizar-documentacao.ts`

```typescript
import fs from 'fs'
import path from 'path'
import { createClient } from '@supabase/supabase-js'

const supabaseUrl = process.env.VITE_SUPABASE_URL!
const supabaseKey = process.env.VITE_SUPABASE_ANON_KEY!

const supabase = createClient(supabaseUrl, supabaseKey)

interface DocumentChunk {
  documento: string;
  secao: string;
  conteudo: string;
  metadata?: any;
}

// Fun√ß√£o para dividir texto em chunks
function dividirTextoEmChunks(texto: string, tamanhoMaximo: number = 1000): string[] {
  const paragrafos = texto.split('\n\n')
  const chunks: string[] = []
  let chunkAtual = ''

  for (const paragrafo of paragrafos) {
    if (chunkAtual.length + paragrafo.length > tamanhoMaximo && chunkAtual.length > 0) {
      chunks.push(chunkAtual.trim())
      chunkAtual = paragrafo
    } else {
      chunkAtual += (chunkAtual ? '\n\n' : '') + paragrafo
    }
  }

  if (chunkAtual.trim()) {
    chunks.push(chunkAtual.trim())
  }

  return chunks
}

// Fun√ß√£o para processar um documento
function processarDocumento(caminhoArquivo: string, nomeDocumento: string): DocumentChunk[] {
  const conteudo = fs.readFileSync(caminhoArquivo, 'utf-8')
  const chunks = dividirTextoEmChunks(conteudo)
  
  return chunks.map((chunk, index) => ({
    documento: nomeDocumento,
    secao: `secao_${index + 1}`,
    conteudo: chunk,
    metadata: {
      arquivo_origem: caminhoArquivo,
      chunk_index: index,
      total_chunks: chunks.length
    }
  }))
}

async function vectorizarDocumentacao() {
  console.log('Iniciando vectoriza√ß√£o da documenta√ß√£o...')

  const documentos = [
    {
      arquivo: 'docs/despesas/documentacao_despesas.md',
      nome: 'documentacao_despesas'
    },
    {
      arquivo: 'docs/orcamentoIA/documentacao_orcamento.md',
      nome: 'documentacao_orcamento'
    },
    {
      arquivo: 'docs/obras/documentacao_obras.md',
      nome: 'documentacao_obras'
    },
    {
      arquivo: 'docs/contrato/documentacao_contratoIA.md',
      nome: 'documentacao_contratoIA'
    }
  ]

  for (const doc of documentos) {
    try {
      console.log(`Processando ${doc.nome}...`)
      
      const caminhoCompleto = path.join(process.cwd(), doc.arquivo)
      
      if (!fs.existsSync(caminhoCompleto)) {
        console.warn(`Arquivo n√£o encontrado: ${caminhoCompleto}`)
        continue
      }

      const chunks = processarDocumento(caminhoCompleto, doc.nome)
      console.log(`Gerados ${chunks.length} chunks para ${doc.nome}`)

      // Chamar Edge Function para gerar embeddings
      const { data, error } = await supabase.functions.invoke('gerar-embeddings-documentacao', {
        body: {
          documento: doc.nome,
          chunks
        }
      })

      if (error) {
        console.error(`Erro ao processar ${doc.nome}:`, error)
        continue
      }

      console.log(`‚úÖ ${doc.nome} processado com sucesso:`, data)
      
    } catch (error) {
      console.error(`Erro ao processar ${doc.nome}:`, error)
    }
  }

  console.log('Vectoriza√ß√£o conclu√≠da!')
}

// Executar se chamado diretamente
if (require.main === module) {
  vectorizarDocumentacao().catch(console.error)
}

export { vectorizarDocumentacao }
```

**Execu√ß√£o:**
```bash
# Instalar depend√™ncias se necess√°rio
npm install @supabase/supabase-js

# Executar script
npx tsx scripts/vectorizar-documentacao.ts
```

### 4. Testar a busca sem√¢ntica

**Objetivo:** Verificar se a busca contextual est√° funcionando corretamente.

**Implementa√ß√£o:**

**Arquivo:** `scripts/testar-busca-semantica.ts`

```typescript
import { createClient } from '@supabase/supabase-js'

const supabaseUrl = process.env.VITE_SUPABASE_URL!
const supabaseKey = process.env.VITE_SUPABASE_ANON_KEY!

const supabase = createClient(supabaseUrl, supabaseKey)

async function testarBuscaSemantica() {
  const consultas = [
    'Como criar um or√ßamento de obra?',
    'Quais s√£o os tipos de despesas em uma obra?',
    'Como funciona o controle de obras?',
    'Documenta√ß√£o sobre contratos de constru√ß√£o'
  ]

  for (const consulta of consultas) {
    console.log(`\nüîç Testando: "${consulta}"`)
    
    try {
      const { data, error } = await supabase.functions.invoke('ai-chat-contextual', {
        body: {
          message: consulta,
          context_type: 'documentacao'
        }
      })

      if (error) {
        console.error('‚ùå Erro:', error)
        continue
      }

      console.log('‚úÖ Resposta:', data.response)
      console.log('üìö Contexto encontrado:', data.context_used ? 'Sim' : 'N√£o')
      
    } catch (error) {
      console.error('‚ùå Erro na consulta:', error)
    }
  }
}

// Executar se chamado diretamente
if (require.main === module) {
  testarBuscaSemantica().catch(console.error)
}

export { testarBuscaSemantica }
```

**Execu√ß√£o:**
```bash
npx tsx scripts/testar-busca-semantica.ts
```

## Configura√ß√µes Necess√°rias

### Vari√°veis de Ambiente no Supabase

Adicionar no painel do Supabase (Settings > Edge Functions > Environment Variables):

```
OPENAI_API_KEY=sk-...
```

### Modifica√ß√µes na Edge Function ai-chat-contextual

Atualizar a fun√ß√£o para usar a nova tabela `embeddings_conhecimento`:

```typescript
// Buscar contexto relevante
const { data: contextData } = await supabase.rpc('buscar_contexto_conhecimento', {
  query_embedding: queryEmbedding,
  match_threshold: 0.8,
  match_count: 5
})
```

### Fun√ß√£o SQL para Busca de Similaridade

```sql
CREATE OR REPLACE FUNCTION buscar_contexto_conhecimento(
  query_embedding vector(1536),
  match_threshold float,
  match_count int
)
RETURNS TABLE (
  documento text,
  secao text,
  conteudo text,
  similarity float
)
LANGUAGE plpgsql
AS $$
BEGIN
  RETURN QUERY
  SELECT 
    embeddings_conhecimento.documento,
    embeddings_conhecimento.secao,
    embeddings_conhecimento.conteudo,
    1 - (embeddings_conhecimento.embedding <=> query_embedding) AS similarity
  FROM embeddings_conhecimento
  WHERE 1 - (embeddings_conhecimento.embedding <=> query_embedding) > match_threshold
  ORDER BY embeddings_conhecimento.embedding <=> query_embedding
  LIMIT match_count;
END;
$$;
```

## Cronograma de Implementa√ß√£o

1. **Dia 1:** Criar tabela e fun√ß√£o SQL
2. **Dia 2:** Implementar Edge Function gerar-embeddings-documentacao
3. **Dia 3:** Executar vectoriza√ß√£o inicial
4. **Dia 4:** Testar e ajustar busca sem√¢ntica
5. **Dia 5:** Documentar e finalizar

## Benef√≠cios Esperados

- **IA Contextual Funcional:** Chatbot com conhecimento espec√≠fico do ObrasAI
- **Respostas Precisas:** Baseadas na documenta√ß√£o oficial
- **Escalabilidade:** F√°cil adi√ß√£o de novos documentos
- **Performance:** Busca sem√¢ntica otimizada

---

**Status:** Pendente de implementa√ß√£o
**Prioridade:** Alta
**Estimativa:** 5 dias de desenvolvimento